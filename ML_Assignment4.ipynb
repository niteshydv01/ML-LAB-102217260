{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niteshydv01/ML-LAB-102217260/blob/main/ML_Assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Q 1 (Based on Step-by-Step Implementation of Ridge Regression using Gradient\n",
        "Descent Optimization)\n",
        "Generate a dataset with atleast seven highly correlated columns and a target variable.\n",
        "Implement Ridge Regression using Gradient Descent Optimization. Take different\n",
        "values of learning rate (such as 0.0001,0.001,0.01,0.1,1,10) and regularization\n",
        "parameter (10-15,10-10,10-5\n",
        ",10- 3\n",
        ",0,1,10,20). Choose the best parameters for which ridge\n",
        "regression cost function is minimum and R2_score is maximum."
      ],
      "metadata": {
        "id": "hNiz8I_zVvIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score"
      ],
      "metadata": {
        "id": "mGfUV76nV2Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "X1 = np.random.rand(n_samples)\n",
        "X2 = X1 + np.random.normal(0, 0.01, n_samples)\n",
        "X3 = X1 + np.random.normal(0, 0.01, n_samples)\n",
        "X4 = X2 + np.random.normal(0, 0.01, n_samples)\n",
        "X5 = X3 + np.random.normal(0, 0.01, n_samples)\n",
        "X6 = X4 + np.random.normal(0, 0.01, n_samples)\n",
        "X7 = X5 + np.random.normal(0, 0.01, n_samples)\n",
        "X = np.column_stack([X1, X2, X3, X4, X5, X6, X7])\n",
        "y = X1 + X2 * 2 + np.random.normal(0, 0.1, n_samples)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "cfrXDIkXQcDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RidgeRegressionGD:\n",
        "    def __init__(self, learning_rate=0.01, n_iters=1000, reg_param=0):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "        self.reg_param = reg_param\n",
        "        self.theta = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Initialize weights\n",
        "        m, n = X.shape\n",
        "        self.theta = np.zeros(n)\n",
        "        y = y.flatten()\n",
        "\n",
        "        # Gradient Descent\n",
        "        for i in range(self.n_iters):\n",
        "            y_pred = X.dot(self.theta)\n",
        "            residuals = y_pred - y\n",
        "            gradient = (X.T.dot(residuals) + self.reg_param * self.theta) / m\n",
        "            self.theta -= self.learning_rate * gradient\n",
        "\n",
        "    def predict(self, X):\n",
        "        return X.dot(self.theta)\n",
        "\n",
        "    def cost_function(self, X, y):\n",
        "        m = len(y)\n",
        "        y_pred = X.dot(self.theta)\n",
        "        cost = (1/(2*m)) * np.sum((y_pred - y)**2) + (self.reg_param/(2*m)) * np.sum(self.theta**2)\n",
        "        return cost"
      ],
      "metadata": {
        "id": "ebeg1lAnQuBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_r2_score = float('-inf')\n",
        "best_params = {'learning_rate': None, 'reg_param': None}\n",
        "best_model = None\n",
        "\n",
        "# Use smaller learning rates and regularization parameters\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1]  # Remove larger values like 1 and 10\n",
        "regularization_params = [1e-3, 0.01, 0.1, 1, 10]  # Focus on reasonable regularization values\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for reg in regularization_params:\n",
        "        model = RidgeRegressionGD(learning_rate=lr, n_iters=10000, reg_param=reg)\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "\n",
        "        y_pred_test = model.predict(X_test_scaled)\n",
        "\n",
        "        # Skip if predictions contain NaNs\n",
        "        if np.isnan(y_pred_test).sum() > 0:\n",
        "            print(f\"Skipping due to NaN values for learning_rate: {lr}, reg_param: {reg}\")\n",
        "            continue\n",
        "\n",
        "        # Compute R2 score\n",
        "        test_r2 = r2_score(y_test, y_pred_test)\n",
        "\n",
        "        # Check for the best model\n",
        "        if test_r2 > best_r2_score:\n",
        "            best_r2_score = test_r2\n",
        "            best_params['learning_rate'] = lr\n",
        "            best_params['reg_param'] = reg\n",
        "            best_model = model\n",
        "\n",
        "print(f\"Best learning rate: {best_params['learning_rate']}\")\n",
        "print(f\"Best regularization parameter: {best_params['reg_param']}\")\n",
        "print(f\"Best R2 score: {best_r2_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aOmlOoGQxkU",
        "outputId": "c248e412-7f95-4dc6-bb8e-d4027673386d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best learning rate: 0.0001\n",
            "Best regularization parameter: 10\n",
            "Best R2 score: -1.8961014083430814\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "***\n",
        "####Q 2 Load the Hitters dataset from the following link\n",
        "https://drive.google.com/file/d/1qzCKF6JKKMB0p7ul_lLy8tdmRk3vE_bG/view?usp=sharing\n",
        "\n",
        "(a) Pre-process the data (null values, noise, categorical to numerical encoding)\n",
        "\n",
        "(b) Separate input and output features and perform scaling\n",
        "\n",
        "(c) Fit a Linear, Ridge (use regularization parameter as 0.5748), and LASSO (use\n",
        "regularization parameter as 0.5748) regression function on the dataset.\n",
        "\n",
        "(d) Evaluate the performance of each trained model on test set. Which model performs\n",
        "the best and Why?"
      ],
      "metadata": {
        "id": "sPTfeTNkV2Q1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "!!!!! provided dataset link is not working !!!!!"
      ],
      "metadata": {
        "id": "RlhO5eYIc4CV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xPJCN1pAc2Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "***\n",
        "####Q 3 Cross Validation for Ridge and Lasso Regression\n",
        "Explore Ridge Cross Validation (RidgeCV) and Lasso Cross Validation (LassoCV)\n",
        "function of Python. Implement both on Boston House Prediction Dataset (load_boston\n",
        "dataset from sklearn.datasets)."
      ],
      "metadata": {
        "id": "wf6Qhhu1XvY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.linear_model import RidgeCV, LassoCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "R0xordx1Y-87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boston = fetch_openml(name='boston', version=1, as_frame=True)\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "8sxnWXXRZCiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alphas = np.logspace(-6, 6, 200)"
      ],
      "metadata": {
        "id": "guqib_45ZC1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ridge_cv = RidgeCV(alphas=alphas)\n",
        "ridge_cv.fit(X_train, y_train)\n",
        "ridge_pred = ridge_cv.predict(X_test)"
      ],
      "metadata": {
        "id": "sCBg7O-9ZAjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso_cv = LassoCV(alphas=alphas, max_iter=10000, cv=5)\n",
        "lasso_cv.fit(X_train, y_train)\n",
        "lasso_pred = lasso_cv.predict(X_test)"
      ],
      "metadata": {
        "id": "9mlbU3GTa9P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ridge_mse = mean_squared_error(y_test, ridge_pred)\n",
        "lasso_mse = mean_squared_error(y_test, lasso_pred)\n",
        "print(\"Optimal alpha for RidgeCV: \", ridge_cv.alpha_)\n",
        "print(\"Mean Squared Error for RidgeCV: \", ridge_mse)\n",
        "print(\"Optimal alpha for LassoCV: \", lasso_cv.alpha_)\n",
        "print(\"Mean Squared Error for LassoCV: \", lasso_mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEs1SzYKa_KA",
        "outputId": "0b996c3e-91b4-4b5b-d287-e274533ff6d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal alpha for RidgeCV:  0.02523539170434766\n",
            "Mean Squared Error for RidgeCV:  24.29287565098284\n",
            "Optimal alpha for LassoCV:  1e-06\n",
            "Mean Squared Error for LassoCV:  24.29111675373595\n"
          ]
        }
      ]
    }
  ]
}